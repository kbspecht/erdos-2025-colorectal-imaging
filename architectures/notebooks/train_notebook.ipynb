{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a8108c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf74d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2a814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b66d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# #original run\n",
    "# !python ../src/train_faster_rcnn.py \\\n",
    "#  --train-json ../artifacts/train.json \\\n",
    "#  --val-json ../artifacts/val.json \\\n",
    "#  --epochs 30 \\\n",
    "#  --profile gpu \\\n",
    "#  --img-size 640 \\\n",
    "#  --batch-size 2 \\\n",
    "#  --train-aug light \\\n",
    "#  --freeze-up-to 2 \\\n",
    "#  --lr 0.002 \\\n",
    "#  --lr-backbone 0.0002 \\\n",
    "#  --num-workers 0 \\\n",
    "#  --scheduler none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd8a07",
   "metadata": {},
   "source": [
    "# RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "062b569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] processing dataset: /Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures/src/faster_rcnn/data/center_stratified/C3\n",
      "[info] detected layout A under src/faster_rcnn/data/center_stratified/C3\n",
      "   train images: True → src/faster_rcnn/data/center_stratified/C3/images/train\n",
      "   train labels: True → src/faster_rcnn/data/center_stratified/C3/labels/train\n",
      "   val images:   True → src/faster_rcnn/data/center_stratified/C3/images/val\n",
      "   val labels:   True → src/faster_rcnn/data/center_stratified/C3/labels/val\n",
      "[info] scanning images in src/faster_rcnn/data/center_stratified/C3/images/train\n",
      "[info] found 314 images in src/faster_rcnn/data/center_stratified/C3/images/train\n",
      "[diag] src/faster_rcnn/data/center_stratified/C3/images/train: images=314 matched_labels=314 boxes=368 skipped=0\n",
      "[info] scanning images in src/faster_rcnn/data/center_stratified/C3/images/val\n",
      "[info] found 39 images in src/faster_rcnn/data/center_stratified/C3/images/val\n",
      "[diag] src/faster_rcnn/data/center_stratified/C3/images/val: images=39 matched_labels=39 boxes=50 skipped=0\n",
      "[info] wrote src/artifacts/train.json\n",
      "[info] wrote src/artifacts/val.json\n",
      "[info] wrote src/artifacts/roots_map.json\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "!python src/faster_rcnn/data/prepare_data_faster_rcnn.py \\\n",
    "    --datasets src/faster_rcnn/data/center_stratified/C3 \\\n",
    "    --out-dir src/artifacts \\\n",
    "    --class-name polyp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ceefa61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\n",
      "env: PYTHONPATH=src:$PYTHONPATH\n",
      "/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures/src/faster_rcnn/augmentations/presets_faster_rcnn.py:312: UserWarning: Argument(s) 'cval' are not valid for transform Affine\n",
      "  A.Affine(\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures/src/faster_rcnn/training/train_faster_rcnn.py:255: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "Train E1:   0%|                                                              | 0/40 [00:00<?, ?it/s]/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Train E1: 100%|█████████████████████████████████████████████████████| 40/40 [35:19<00:00, 52.99s/it]\n",
      "[Epoch 001] lr=0.000100 | train_loss=0.3488 | val_loss=0.2105\n",
      "Train E2: 100%|█████████████████████████████████████████████████████| 40/40 [13:58<00:00, 20.97s/it]\n",
      "[Epoch 002] lr=0.000100 | train_loss=0.1892 | val_loss=0.1730\n",
      "Train E3: 100%|█████████████████████████████████████████████████████| 40/40 [14:03<00:00, 21.10s/it]\n",
      "[Epoch 003] lr=0.000100 | train_loss=0.1658 | val_loss=0.1622\n",
      "Train E4: 100%|█████████████████████████████████████████████████████| 40/40 [13:43<00:00, 20.60s/it]\n",
      "[Epoch 004] lr=0.000100 | train_loss=0.1654 | val_loss=0.1446\n",
      "Train E5: 100%|█████████████████████████████████████████████████████| 40/40 [13:03<00:00, 19.58s/it]\n",
      "[Epoch 005] lr=0.000100 | train_loss=0.1481 | val_loss=0.1484\n",
      "Train E6: 100%|█████████████████████████████████████████████████████| 40/40 [13:13<00:00, 19.84s/it]\n",
      "[Epoch 006] lr=0.000100 | train_loss=0.1525 | val_loss=0.1371\n",
      "Train E7: 100%|█████████████████████████████████████████████████████| 40/40 [13:59<00:00, 20.99s/it]\n",
      "[Epoch 007] lr=0.000100 | train_loss=0.1383 | val_loss=0.1399\n",
      "Train E8: 100%|█████████████████████████████████████████████████████| 40/40 [55:56<00:00, 83.91s/it]\n",
      "[Epoch 008] lr=0.000100 | train_loss=0.1441 | val_loss=0.1318\n",
      "Train E9: 100%|█████████████████████████████████████████████████████| 40/40 [13:46<00:00, 20.66s/it]\n",
      "[Epoch 009] lr=0.000100 | train_loss=0.1340 | val_loss=0.1343\n",
      "Train E10:   5%|██▋                                                  | 2/40 [00:45<14:24, 22.76s/it]^C\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 105, in forward\n",
      "    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torchvision/models/detection/roi_heads.py\", line 761, in forward\n",
      "    box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torchvision/ops/poolers.py\", line 314, in forward\n",
      "    return _multiscale_roi_align(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torchvision/ops/poolers.py\", line 204, in _multiscale_roi_align\n",
      "    result_idx_in_level = roi_align(\n",
      "                          ^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torchvision/ops/roi_align.py\", line 258, in roi_align\n",
      "    return torch.ops.torchvision.roi_align(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n",
      "    return self._op(*args, **(kwargs or {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\n",
    "%env PYTHONPATH=src:$PYTHONPATH\n",
    "\n",
    "!python -m faster_rcnn.training.train_faster_rcnn \\\n",
    "    --train-json src/artifacts/train.json \\\n",
    "    --val-json src/artifacts/val.json \\\n",
    "    --roots-map src/artifacts/roots_map.json \\\n",
    "    --images-root src/faster_rcnn/data/center_stratified/C3 \\\n",
    "    --epochs 20 \\\n",
    "    --batch-size 8 \\\n",
    "    --num-workers 0 \\\n",
    "    --img-size 640 \\\n",
    "    --train-augs medium \\\n",
    "    --num-classes 2 \\\n",
    "    --freeze-backbone 2 \\\n",
    "    --opt sgd \\\n",
    "    --lr-backbone 1e-4 \\\n",
    "    --lr-heads 5e-3 \\\n",
    "    --weight-decay 1e-4 \\\n",
    "    --lr-scheduler none \\\n",
    "    --device cpu\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c69dbf",
   "metadata": {},
   "source": [
    "# STOP HERE FOR INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6a6df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys BEFORE: dict_keys(['images', 'annotations', 'categories', 'info', 'licenses'])\n",
      "keys AFTER: dict_keys(['images', 'annotations', 'categories', 'info', 'licenses'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\")\n",
    "val_json_path   = root / \"src\" / \"artifacts\" / \"val.json\"\n",
    "\n",
    "with val_json_path.open(\"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"keys BEFORE:\", data.keys())\n",
    "\n",
    "# Add missing keys for pycocotools\n",
    "if \"info\" not in data:\n",
    "    data[\"info\"] = {\"description\": \"polyp dataset\"}\n",
    "if \"licenses\" not in data:\n",
    "    data[\"licenses\"] = []\n",
    "\n",
    "with val_json_path.open(\"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "print(\"keys AFTER:\", data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "250f22ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO eval:   0%|                                                             | 0/20 [00:00<?, ?it/s]/Users/rebekaheichberg/anaconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "COCO eval: 100%|█████████████████████████████████████████████████| 20/20 [1:35:08<00:00, 285.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.019\n",
      "COCO AP=0.0002, AP50=0.0006, AP75=0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add your src folder to Python path\n",
    "SRC = Path(\"/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures/src\")\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "\n",
    "from faster_rcnn.data.data_loaders_faster_rcnn import build_train_val_loaders\n",
    "from faster_rcnn.data.coco_eval_faster_rcnn import coco_map\n",
    "from faster_rcnn.models.faster_rcnn_model import build_fasterrcnn\n",
    "\n",
    "# --- paths ---\n",
    "root = Path(\"/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\")\n",
    "ckpt_path = root / \"runs\" / \"frcnn_polyp\" /\"epoch_009.pth\"\n",
    "train_json = root / \"src\" / \"artifacts\" / \"train.json\"\n",
    "val_json   = root / \"src\" / \"artifacts\" / \"val.json\"\n",
    "roots_map  = root / \"src\" / \"artifacts\" / \"roots_map.json\"\n",
    "images_root = root / \"faster_rcnn\" / \"data\" / \"center_stratified\" / \"C3\" / \"images\" \n",
    "# --- setup model ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = build_fasterrcnn(num_classes=2, pretrained=False)\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state[\"model\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- build validation loader ---\n",
    "# (these args match your build_train_val_loaders signature)\n",
    "_, val_loader = build_train_val_loaders(\n",
    "    train_json=str(train_json),\n",
    "    val_json=str(val_json),\n",
    "    roots_map=str(roots_map),\n",
    "    images_root=str(images_root),\n",
    "    img_size=832,         # match your training size\n",
    "    train_augs=\"none\",    # val loader ignores this anyway\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# --- run COCO eval ---\n",
    "stats = coco_map(model, val_loader, device, coco_gt_json=str(val_json))\n",
    "print(f\"COCO AP={stats[0]:.4f}, AP50={stats[1]:.4f}, AP75={stats[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ba849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\")  # or whatever it is\n",
    "\n",
    "ckpt = root / \"runs\" / \"frcnn_polyp\" / \"epoch_009.pth\"\n",
    "images_dir = root / \"src\" / \"faster_rcnn\" / \"data\" / \"center_stratified\" / \"C3\" / \"images\" / \"val\" \n",
    "out_dir = root / \"runs\" /\"frcnn_polyp\" / \"inference_side_by_side_rebekah\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071a3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\n",
      "[info] using device: cpu\n",
      "[info] building model...\n",
      "[info] weights loaded.\n",
      "[info] loading COCO GT from src/artifacts/val.json\n",
      "[info] found 39 images.\n",
      "[1/39] c3s000p4_0d17_C3_EndoCV2021_00180.jpg\n",
      "    GT annos: 4\n",
      "[2/39] c3s001p1_24d7_C3_EndoCV2021_00150.jpg\n",
      "    GT annos: 1\n",
      "[3/39] c3s002p5_0d18_C3_EndoCV2021_00369.jpg\n",
      "    GT annos: 5\n",
      "[4/39] c3s003p1_2d63_C3_EndoCV2021_00167.jpg\n",
      "    GT annos: 1\n",
      "[5/39] c3s004p1_23d2_C3_EndoCV2021_00245.jpg\n",
      "    GT annos: 1\n",
      "[6/39] c3s005p1_63d8_C3_EndoCV2021_00275.jpg\n",
      "    GT annos: 1\n",
      "[7/39] c3s006p1_1d58_C3_EndoCV2021_00139.jpg\n",
      "    GT annos: 1\n",
      "[8/39] c3s007p1_35d3_C3_EndoCV2021_00233.jpg\n",
      "    GT annos: 1\n",
      "[9/39] c3s008p1_27d8_C3_EndoCV2021_00327.jpg\n",
      "    GT annos: 1\n",
      "[10/39] c3s009p1_21d5_C3_EndoCV2021_00119.jpg\n",
      "    GT annos: 1\n",
      "[11/39] c3s010p2_4d01_C3_EndoCV2021_00142.jpg\n",
      "    GT annos: 2\n",
      "[12/39] c3s011p1_16d0_C3_EndoCV2021_00101.jpg\n",
      "    GT annos: 1\n",
      "[13/39] c3s012p1_5d14_C3_EndoCV2021_00371.jpg\n",
      "    GT annos: 1\n",
      "[14/39] c3s013p1_31d8_C3_EndoCV2021_00130.jpg\n",
      "    GT annos: 1\n",
      "[15/39] c3s014p1_1d07_C3_EndoCV2021_00240.jpg\n",
      "    GT annos: 1\n",
      "[16/39] c3s015p1_1d89_C3_EndoCV2021_00175.jpg\n",
      "    GT annos: 1\n",
      "[17/39] c3s016p1_19d4_C3_EndoCV2021_00207.jpg\n",
      "    GT annos: 1\n",
      "[18/39] c3s017p1_4d53_C3_EndoCV2021_00236.jpg\n",
      "    GT annos: 1\n",
      "[19/39] c3s018p1_7d57_C3_EndoCV2021_00237.jpg\n",
      "    GT annos: 1\n",
      "[20/39] c3s019p1_6d44_C3_EndoCV2021_00212.jpg\n",
      "    GT annos: 1\n",
      "[21/39] c3s020p2_0d80_C3_EndoCV2021_00262.jpg\n",
      "    GT annos: 2\n",
      "[22/39] c3s021p1_7d50_C3_EndoCV2021_003.jpg\n",
      "    GT annos: 1\n",
      "[23/39] c3s022p1_32d1_C3_EndoCV2021_00386.jpg\n",
      "    GT annos: 1\n",
      "[24/39] c3s023p1_8d67_C3_EndoCV2021_00455.jpg\n",
      "    GT annos: 1\n",
      "[25/39] c3s024p1_12d3_C3_EndoCV2021_00129.jpg\n",
      "    GT annos: 1\n",
      "[26/39] c3s025p1_10d7_C3_EndoCV2021_00270.jpg\n",
      "    GT annos: 1\n",
      "[27/39] c3s026p1_18d7_C3_EndoCV2021_00103.jpg\n",
      "    GT annos: 1\n",
      "[28/39] c3s027p1_16d5_C3_EndoCV2021_00152.jpg\n",
      "    GT annos: 1\n",
      "[29/39] c3s028p1_1d12_C3_EndoCV2021_00170.jpg\n",
      "    GT annos: 1\n",
      "[30/39] c3s029p1_6d74_C3_EndoCV2021_00372.jpg\n",
      "    GT annos: 1\n",
      "[31/39] c3s030p1_25d2_C3_EndoCV2021_00127.jpg\n",
      "    GT annos: 1\n",
      "[32/39] c3s031p3_0d23_C3_EndoCV2021_00485.jpg\n",
      "    GT annos: 3\n",
      "[33/39] c3s032p1_7d82_C3_EndoCV2021_00335.jpg\n",
      "    GT annos: 1\n",
      "[34/39] c3s033p1_42d0_C3_EndoCV2021_001.jpg\n",
      "    GT annos: 1\n",
      "[35/39] c3s034p1_8d95_C3_EndoCV2021_00112.jpg\n",
      "    GT annos: 1\n",
      "[36/39] c3s035p1_3d29_C3_EndoCV2021_00193.jpg\n",
      "    GT annos: 1\n",
      "[37/39] c3s036p1_0d74_C3_EndoCV2021_00182.jpg\n",
      "    GT annos: 1\n",
      "[38/39] c3s037p1_6d77_C3_EndoCV2021_00466.jpg\n",
      "    GT annos: 1\n",
      "[39/39] c3s038p1_13d5_C3_EndoCV2021_00151.jpg\n",
      "    GT annos: 1\n",
      "[done] saved visualizations to: /Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures/runs/frcnn_polyp/inference_side_by_side_rebekah\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/rebekaheichberg/Desktop/git-re/colorectal_imaging/Architectures\n",
    "\n",
    "!python src/faster_rcnn/inference/infer_folder_faster_rcnn.py \\\n",
    "    --weights \"{ckpt}\" \\\n",
    "    --images-dir \"{images_dir}\" \\\n",
    "    --out-dir \"{out_dir}\" \\\n",
    "    --coco-json src/artifacts/val.json \\\n",
    "    --num-classes 2 \\\n",
    "    --score-thr 0.5 \\\n",
    "    --nms-iou 0.4 \\\n",
    "    --debug-gt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
